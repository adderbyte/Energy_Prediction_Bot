{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ copyright\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import gc\n",
    "warnings.simplefilter('ignore')\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "### to save data or model\n",
    "import pickle\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     The aim of this notebook is to provide investogation of the data sets\n",
    "     as a first requirement for building a model. Note that there was computationally limited resources (using a    PC).So, I always think of the simplest and cost effective way to go about achieving a task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "\n",
    "[If you have the merged data set file then click here to skip this part](#Merged-Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Note that the test data have been committed out but similar steps would be replicated for the test data. </p>\n",
    "\n",
    " The goal here is to prepare the training data set for\n",
    " training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the training set\n",
    "train = pd.read_csv('train.csv')\n",
    "#test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the building data set\n",
    "building = pd.read_csv('building_metadata.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Merging Train and building data sets that have been imported from above.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(building, on='building_id', how='left')\n",
    "#test = test.merge(building, on='building_id', how='left')\n",
    "#test.to_pickle('test_building')  # where to save it, usually as a .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data set as pickle file to enable easy loading of merged data set\n",
    "# This also saves the cost of merging\n",
    "train.to_pickle('train_building')  # where to save it, usually as a .pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load merged building and train data\n",
    "mergerd = pd.read_pickle('train_building')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weather data\n",
    "weather_train = pd.read_csv('weather_train.csv')\n",
    "#weather_test = pd.read_csv('weather_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged to produce train-building-weather data set\n",
    "train = mergerd.merge(weather_train, on=['site_id', 'timestamp'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_id', 'meter', 'timestamp', 'meter_reading', 'site_id',\n",
       "       'primary_use', 'square_feet', 'year_built', 'floor_count',\n",
       "       'air_temperature', 'cloud_coverage', 'dew_temperature',\n",
       "       'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction',\n",
       "       'wind_speed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of columns present in data set\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us investigate if it is possible to parition data based on meter, site_id and primary_use \n",
    "\n",
    "    1) We have to check the  unique elements present in each of the 3 columns named above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Education', 'Lodging/residential', 'Office',\n",
       "       'Entertainment/public assembly', 'Other', 'Retail', 'Parking',\n",
       "       'Public services', 'Warehouse/storage', 'Food sales and service',\n",
       "       'Religious worship', 'Healthcare', 'Utility', 'Technology/science',\n",
       "       'Manufacturing/industrial', 'Services'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique elements  in the column 'primary_use\n",
    "train.primary_use.unique() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 1, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique element in the column meter \n",
    "train.meter.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique element in the column site id\n",
    "train.site_id.unique() # the numbers of site id present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> we observe that this will create a lot of smaller subset and it might not be possible to make a model for each\n",
    "    of this (for computational limitation and flexibility reasons) </p>\n",
    "    \n",
    "Nevertheless we will proceed to see how the partitioning could be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning\n",
    "    \n",
    "    1) Primary_use column is an pandas object type (or say it is not a numeric value).\n",
    "    2) Convert it to pandas category type and  assign a numeric value to each category. The category mappings              can be trivially retrieved in pandas\n",
    "    3) Meter and site_id are both int64 type. Therefore nopthing to be done here\n",
    "    4) It is possible to then use Multi index approach to index the data set on primery_use,\n",
    "       meter and site_id\n",
    "    5) However, let us use a simple approach. A new column will be added to the data set and it\n",
    "       will be called separator. To achieve this convert meter, site_id, and primary_use to string data type.\n",
    "    6) The column separator is defined as:\n",
    "          separator  = meter + primary_use  + site_id\n",
    "    7) Now we can select a subset or partition based on this selector. Data points belonging to the same\n",
    "       separator value would share same meter id, primary use and site id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 1 : convert pry_use to category set to \n",
    "train[\"primary_use\"] = train[\"primary_use\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "building_id              int64\n",
       "meter                    int64\n",
       "timestamp               object\n",
       "meter_reading          float64\n",
       "site_id                  int64\n",
       "primary_use           category\n",
       "square_feet              int64\n",
       "year_built             float64\n",
       "floor_count            float64\n",
       "air_temperature        float64\n",
       "cloud_coverage         float64\n",
       "dew_temperature        float64\n",
       "precip_depth_1_hr      float64\n",
       "sea_level_pressure     float64\n",
       "wind_direction         float64\n",
       "wind_speed             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that primary use is of type category\n",
    "# this step does not change the label for the contents of pimary  use\n",
    "train.dtypes # primary use is now a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode each category as numeric values\n",
    "train[\"primary_use_encodings\"] = train[\"primary_use\"].cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  4,  6,  1,  7, 11,  8,  9, 15,  2, 10,  3, 14, 13,  5, 12])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cheeck the encodings oe mappings that have been assigned\n",
    "train.primary_use_encodings.unique() # the encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now step 5\n",
    "# step 5 : convert meter from int64 to string\n",
    "train['meter']= train['meter'].values.astype(np.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert site_id to string too\n",
    "train['site_id']= train['site_id'].values.astype(np.str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pry use  to string\n",
    "train['primary_use_encodings']= train['primary_use_encodings'].values.astype(np.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 6 : separator column defined here \n",
    "train['separator'] = train['meter'] + train['site_id']+ train['primary_use_encodings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_partition = train.separator.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000', '004', '006', '001', '007', '0011', '008', '010', '310',\n",
       "       '014', '011', '019', '319', '016', '316', '029', '026', '020',\n",
       "       '021', '120', '320', '0215', '024', '124', '324', '0211', '1211',\n",
       "       '126', '326', '121', '321', '022', '122', '322', '0210', '129',\n",
       "       '028', '329', '023', '123', '0214', '1214', '039', '036', '031',\n",
       "       '030', '034', '033', '0311', '0315', '038', '037', '0310', '040',\n",
       "       '0413', '044', '049', '041', '0414', '048', '057', '050', '051',\n",
       "       '053', '054', '055', '059', '056', '060', '260', '066', '160',\n",
       "       '064', '266', '264', '061', '161', '261', '164', '069', '170',\n",
       "       '270', '070', '370', '086', '081', '087', '089', '0815', '096',\n",
       "       '196', '094', '194', '294', '090', '290', '190', '091', '191',\n",
       "       '291', '0912', '099', '199', '299', '296', '0100', '1100', '3100',\n",
       "       '0107', '1107', '0101', '0106', '01013', '11013', '31013', '3101',\n",
       "       '0104', '0110', '3110', '1110', '0120', '01211', '0126', '01213',\n",
       "       '0129', '0121', '0136', '0134', '1134', '1133', '2133', '1136',\n",
       "       '2136', '2134', '1131', '2131', '0130', '1130', '2130', '0135',\n",
       "       '0138', '0137', '0139', '1139', '2139', '0132', '2132', '2138',\n",
       "       '01312', '21312', '2135', '0131', '01313', '21315', '1138', '2137',\n",
       "       '01315', '0133', '0146', '1146', '3146', '2146', '0140', '1140',\n",
       "       '2140', '3140', '0149', '1149', '3149', '0143', '1143', '2143',\n",
       "       '3143', '0142', '1142', '2142', '3142', '0141', '1141', '3141',\n",
       "       '0144', '1144', '2149', '2144', '3144', '2141', '3151', '0156',\n",
       "       '1156', '2156', '0150', '1150', '01513', '11513', '21513', '2150',\n",
       "       '3150', '0159', '0155', '1155', '01510', '11510', '0154', '1154',\n",
       "       '2154', '0151', '2151', '01514', '21514', '1151', '1159', '2159',\n",
       "       '1153', '2155', '166', '169', '100', '106', '1011', '107', '104'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the unique partions or groups available\n",
    "# it is very diffcult to create a model for each of these\n",
    "# read this as 000: meter 0 , site 0, primary use 0 (education)\n",
    "# 004 : meter 0 , site id 0, pry use 4 (Lodging/residential)\n",
    "# to get the mapping of pry use check next cell\n",
    "unique_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mappings for primary use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    " # store the encodings/numbers  of the pry use\n",
    "labels = train.primary_use.unique() #munique elements of the sites (how many sites we have)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each number retrieve its category and store as tuple in a list\n",
    "\n",
    "category_mappings=[] # list for storing number and cateogory pair\n",
    "for i in labels:\n",
    "    temp = train.primary_use.cat.categories.get_loc(i) # retrieve the category given the number\n",
    "    category_mappings.append((temp,i)) # store the mappings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Education'),\n",
       " (4, 'Lodging/residential'),\n",
       " (6, 'Office'),\n",
       " (1, 'Entertainment/public assembly'),\n",
       " (7, 'Other'),\n",
       " (11, 'Retail'),\n",
       " (8, 'Parking'),\n",
       " (9, 'Public services'),\n",
       " (15, 'Warehouse/storage'),\n",
       " (2, 'Food sales and service'),\n",
       " (10, 'Religious worship'),\n",
       " (3, 'Healthcare'),\n",
       " (14, 'Utility'),\n",
       " (13, 'Technology/science'),\n",
       " (5, 'Manufacturing/industrial'),\n",
       " (12, 'Services')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all the mappings categories\n",
    "category_mappings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data as csv\n",
    "train.to_csv('AllData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_csv('AllData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have 0112, it might be difficult to know whether the site id is 1 and category is 12 or vice versa.\n",
    "So, I have decided to convert site id to alphabets to remove this ambiguity.\n",
    "The result of this is same as when site id is not chneged but this is easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site_id as Alphabets\n",
    "train[\"site_id\"] = train[\"site_id\"].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#siteIdUnique = train[\"site_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " # this is what site id will be mapped to\n",
    "newSiteName = ['a','b','c','d','e','f','g','h','i','j', 'k', 'l','m','n','o','p']\n",
    "# change site id to category.and assign the mapping from the list aboce\n",
    "train.site_id.cat.rename_categories(newSiteName,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site id was chneged to str for concatenation\n",
    "train['site_id']= train['site_id'].values.astype(np.str) # convert siteid back to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete previous separator\n",
    "del train['separator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new separator column\n",
    "train['separator'] = train['meter'] + train['site_id']+ train['primary_use_encodings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the unique separator values which corresponds to the partiton\n",
    "separator = train.separator.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0a0', '0a4', '0a6', '0a1', '0a7', '0a11', '0a8', '0b0', '3b0',\n",
       "       '0b4', '0b1', '0b9', '3b9', '0b6', '3b6', '0c9', '0c6', '0c0',\n",
       "       '0c1', '1c0', '3c0', '0c15', '0c4', '1c4', '3c4', '0c11', '1c11',\n",
       "       '1c6', '3c6', '1c1', '3c1', '0c2', '1c2', '3c2', '0c10', '1c9',\n",
       "       '0c8', '3c9', '0c3', '1c3', '0c14', '1c14', '0d9', '0d6', '0d1',\n",
       "       '0d0', '0d4', '0d3', '0d11', '0d15', '0d8', '0d7', '0d10', '0e0',\n",
       "       '0e13', '0e4', '0e9', '0e1', '0e14', '0e8', '0f7', '0f0', '0f1',\n",
       "       '0f3', '0f4', '0f5', '0f9', '0f6', '0g0', '2g0', '0g6', '1g0',\n",
       "       '0g4', '2g6', '2g4', '0g1', '1g1', '2g1', '1g4', '0g9', '1h0',\n",
       "       '2h0', '0h0', '3h0', '0i6', '0i1', '0i7', '0i9', '0i15', '0j6',\n",
       "       '1j6', '0j4', '1j4', '2j4', '0j0', '2j0', '1j0', '0j1', '1j1',\n",
       "       '2j1', '0j12', '0j9', '1j9', '2j9', '2j6', '0k0', '1k0', '3k0',\n",
       "       '0k7', '1k7', '0k1', '0k6', '0k13', '1k13', '3k13', '3k1', '0k4',\n",
       "       '0l0', '3l0', '1l0', '0m0', '0m11', '0m6', '0m13', '0m9', '0m1',\n",
       "       '0n6', '0n4', '1n4', '1n3', '2n3', '1n6', '2n6', '2n4', '1n1',\n",
       "       '2n1', '0n0', '1n0', '2n0', '0n5', '0n8', '0n7', '0n9', '1n9',\n",
       "       '2n9', '0n2', '2n2', '2n8', '0n12', '2n12', '2n5', '0n1', '0n13',\n",
       "       '2n15', '1n8', '2n7', '0n15', '0n3', '0o6', '1o6', '3o6', '2o6',\n",
       "       '0o0', '1o0', '2o0', '3o0', '0o9', '1o9', '3o9', '0o3', '1o3',\n",
       "       '2o3', '3o3', '0o2', '1o2', '2o2', '3o2', '0o1', '1o1', '3o1',\n",
       "       '0o4', '1o4', '2o9', '2o4', '3o4', '2o1', '3p1', '0p6', '1p6',\n",
       "       '2p6', '0p0', '1p0', '0p13', '1p13', '2p13', '2p0', '3p0', '0p9',\n",
       "       '0p5', '1p5', '0p10', '1p10', '0p4', '1p4', '2p4', '0p1', '2p1',\n",
       "       '0p14', '2p14', '1p1', '1p9', '2p9', '1p3', '2p5', '1g6', '1g9',\n",
       "       '1a0', '1a6', '1a11', '1a7', '1a4'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the partition\n",
    "separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(separator) # total groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retieve a particular partition use the syntax below\n",
    "# I ran out of memory here so I cant run to show sample results\n",
    "# train.query('separator == \"0f1\"')[1:2]\n",
    "# train.loc[train['separator'] == '0f1'][1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isuse if there are enough computational resources. It might be possible to make plots and compare partions\n",
    "But in the next step the data set will be divided based on meter in order to have a smaller subset and build models \n",
    "with those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
